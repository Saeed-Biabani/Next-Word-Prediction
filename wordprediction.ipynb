{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XK_QTYjoTXzJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.functional import one_hot\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchtext\n",
        "import random\n",
        "import torch\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "uibX6oMTTaPS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReadDataFile(fname):\n",
        "    with open(fname, 'r') as f:\n",
        "        return f.read().split('\\n')"
      ],
      "metadata": {
        "id": "sh0dFzuvTcJ-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "data = ReadDataFile(\"/content/DataSet.txt\")"
      ],
      "metadata": {
        "id": "u1XyACZSTdQ7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokkenized_data = [tokenizer(item) for item in tqdm.tqdm(data)]"
      ],
      "metadata": {
        "id": "b7cw5ACDTfZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    tokkenized_data,\n",
        "    min_freq=2,\n",
        "    specials=['<pad>', '<oov>'],\n",
        "    special_first=True\n",
        ")\n",
        "\n",
        "target_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    tokkenized_data,\n",
        "    min_freq=2\n",
        ")"
      ],
      "metadata": {
        "id": "NG-qVJXETkZ-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_vocab_total_words = len(features_vocab)\n",
        "target_vocab_total_words = len(target_vocab)"
      ],
      "metadata": {
        "id": "o4I8fn58TmGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_numerical_sequence(tokenized_text):\n",
        "    tokens_list = []\n",
        "    if tokenized_text[-1] in target_vocab.get_itos():\n",
        "        for token in tokenized_text[:-1]:\n",
        "            num_token = features_vocab[token] if token in features_vocab.get_itos() else features_vocab['<oov>']\n",
        "            tokens_list.append(num_token)\n",
        "        num_token = target_vocab[tokenized_text[-1]]\n",
        "        tokens_list.append(num_token)\n",
        "        return tokens_list\n",
        "    return None"
      ],
      "metadata": {
        "id": "qgeUSnrrTnYW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_ngrams(tokenized_title):\n",
        "    list_ngrams = []\n",
        "    for i in range(1, len(tokenized_title)):\n",
        "        ngram_sequence = tokenized_title[:i+1]\n",
        "        list_ngrams.append(ngram_sequence)\n",
        "    return list_ngrams"
      ],
      "metadata": {
        "id": "b66VsfXkTomc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_random_oov_tokens(ngram):\n",
        "  row = ngram.copy()\n",
        "  for idx, word in enumerate(row[:-1]):\n",
        "    if random.uniform(0, 1) < 0.1:\n",
        "      row[idx] = '<oov>'\n",
        "  return row"
      ],
      "metadata": {
        "id": "BNlVwAVTTqH7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams_list = []\n",
        "for tokenized_title in tokkenized_data:\n",
        "    ngrams_list.extend(make_ngrams(tokenized_title))"
      ],
      "metadata": {
        "id": "8Aa-BUd-TrH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams_list_oov = []\n",
        "for ngram in ngrams_list:\n",
        "    ngrams_list_oov.append(add_random_oov_tokens(ngram))"
      ],
      "metadata": {
        "id": "Wm6kkibtTsGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sequence in tqdm.tqdm(ngrams_list_oov):\n",
        "  row = text_to_numerical_sequence(sequence)\n",
        "  if row:\n",
        "    input_sequences.append(row)"
      ],
      "metadata": {
        "id": "tPhkp3fSVe2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [sequence[:-1] for sequence in input_sequences]\n",
        "y = [sequence[-1] for sequence in input_sequences]"
      ],
      "metadata": {
        "id": "V_RDZukYWFSh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_sequence_feature = len(max(X, key = len))"
      ],
      "metadata": {
        "id": "mQApMstGWJ14"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_X = [F.pad(torch.tensor(sequence), (longest_sequence_feature - len(sequence),0), value=0) for sequence in tqdm.tqdm(X)]"
      ],
      "metadata": {
        "id": "a44ZTvrKWQQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_tX = torch.stack(padded_X)"
      ],
      "metadata": {
        "id": "OtcTQx_OZ6GK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor(y)\n",
        "y_one_hot = one_hot(y, num_classes=target_vocab_total_words)"
      ],
      "metadata": {
        "id": "683mu0YNZzFC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = TensorDataset(padded_tX, y_one_hot)\n",
        "data_loader = DataLoader(ds, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "MwLovt6SbkLn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class My_LSTM(nn.Module):\n",
        "    def __init__(self, features_vocab_total_words, target_vocab_total_words, embedding_dim, hidden_dim):\n",
        "        super(My_LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(features_vocab_total_words, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_dim, target_vocab_total_words)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.embedding.weight.device)\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "id": "iYzNYfp9byS1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "83uKXlofcFzS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = My_LSTM(features_vocab_total_words, target_vocab_total_words, 256, 512).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "hLZRvc7nb8DB"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_topk_accuracy(model, data_loader, k=3):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loop = tqdm.tqdm(data_loader)\n",
        "        for batch_x, batch_y in loop:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            output = model(batch_x)\n",
        "\n",
        "            _, predicted_indices = output.topk(k, dim=1)\n",
        "\n",
        "            correct_predictions += torch.any(predicted_indices == torch.argmax(batch_y, dim=1, keepdim=True), dim=1).sum().item()\n",
        "            total_predictions += batch_y.size(0)\n",
        "\n",
        "            accuracy = correct_predictions / total_predictions\n",
        "            log = {\n",
        "                \"K-Accuracy\":accuracy*100\n",
        "            }\n",
        "            loop.set_postfix(log)\n",
        "    return accuracy*100"
      ],
      "metadata": {
        "id": "-1WnJDJbdosv"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss = []\n",
        "acc = []\n",
        "for epoch in range(1, 25):\n",
        "    model.train()\n",
        "    loop = tqdm.tqdm(data_loader)\n",
        "    epoch_loss = []\n",
        "    for batch_X, batch_y in loop:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = loss_fn(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        log_ = {\n",
        "            \"epoch\":epoch,\n",
        "            \"loss\" : loss.item()\n",
        "        }\n",
        "        loop.set_postfix(log_)\n",
        "        epoch_loss.append(loss.cpu().detach())\n",
        "    total_loss.append(np.mean(epoch_loss))\n",
        "    acc.append(calculate_topk_accuracy(model, data_loader))\n",
        "torch.save(model.state_dict(), \"next_word_prediction.pth\")"
      ],
      "metadata": {
        "id": "GO84sMZOcHqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(total_loss, \"r\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"CrossEntropy Loss\", loc = \"center\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(acc, \"g\")\n",
        "plt.title(\"Train Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0.25)"
      ],
      "metadata": {
        "id": "IFtCW80hIkCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_state_dict(torch.load(\"/content/next_word_prediction.pth\"))"
      ],
      "metadata": {
        "id": "YwLWAjHq7UdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_numerical_sequence_test(tokenized_text):\n",
        "    tokens_list = []\n",
        "    for token in tokenized_text:\n",
        "        num_token = features_vocab[token] if token in features_vocab.get_itos() else features_vocab['<oov>']\n",
        "        tokens_list.append(num_token)\n",
        "    return tokens_list"
      ],
      "metadata": {
        "id": "Ct1p3ELzknQG"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Prediction(query):\n",
        "  model.eval()\n",
        "\n",
        "  string, num_generation = query\n",
        "  with torch.no_grad():\n",
        "    for i in range(num_generation):\n",
        "      tokenized_str = tokenizer(string)\n",
        "      seq = text_to_numerical_sequence_test(tokenized_str)\n",
        "      padded_tokenized_sequence_input_test = F.pad(torch.tensor(seq), (longest_sequence_feature - len(seq)-1, 0),value=0)\n",
        "      output_test_walking = torch.argmax(model(padded_tokenized_sequence_input_test.unsqueeze(0)))\n",
        "      string = string + ' ' + target_vocab.lookup_token(output_test_walking.item())\n",
        "  return string"
      ],
      "metadata": {
        "id": "y48ccpnqK9ZM"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_1 = [\"این گونه از کرکسها\", 5]\n",
        "best_2 = [\"هواپیماهای\", 6]\n",
        "best_3 = [\"تحقیقات\", 6]\n",
        "best_4 = [\"همشهری\", 10]\n",
        "best_5 = [\"تیم\", 10]\n",
        "best_6 = [\"سیاسی\", 19]\n",
        "best_7 = [\"اجتماعی\", 9]\n",
        "best_8 = [\"مجازی\", 15]\n",
        "best_9 = [\"هجدهم این ماه\", 7]\n",
        "best_10 = [\"افزایش قیمتها\", 3]\n",
        "best_11 = [\"کادرفنی\", 9]\n",
        "best_12 = [\"گفتوگو\", 18]\n",
        "best_13 = [\"سرمربی تیم ملی\", 15]"
      ],
      "metadata": {
        "id": "dT_byamMOIfH"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_ = Prediction(best_1)\n",
        "print(string_)"
      ],
      "metadata": {
        "id": "pUDQevmdcdfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}